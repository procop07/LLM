# GPT_OSS_MXFP4_(20B)-Inference.ipynb - Colab

File

Edit

View

Insert

Runtime

Tools

Help

 Share 

Commands Code Text Copy to Drive  

To run this, press "_Runtime_" and press "_Run all_" on a **free** Tesla T4 Google Colab instance!

   Join Discord if you need help + ⭐ _Star us on [Github]()_ ⭐

To install Unsloth on your own computer, follow the installation instructions on our Github page [here]().

You will learn how to do [data prep](), how to [train](), how to [run the model](), & [how to save it]()

### News

**NEW** Unsloth now supports training the new **gpt-oss** model from OpenAI! You can start finetune gpt-oss for free with our **[Colab notebook]()**!

Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here]().

Read our **[Gemma 3N Guide]()** and check out our new **[Dynamic 2.0]()** quants which outperforms other quantization methods!

Visit our docs for all our [model uploads]() and [notebooks]().

### Installation

### Unsloth

We're about to demonstrate the power of the new OpenAI GPT-OSS 20B model through an inference example. For our `bnb-4bit` version, use this [notebook]() instead.

**We're using OpenAI's MXFP4 Triton kernels combined with Unsloth's kernels!**

from unsloth import FastLanguageModel  
import torch  
  
# 4bit pre quantized models we support for 4x faster downloading + no OOMs.  
fourbit\_models = \[  
    "unsloth/gpt-oss-20b-unsloth-bnb-4bit", # 20B model using bitsandbytes 4bit quantization  
    "unsloth/gpt-oss-120b-unsloth-bnb-4bit",  
    "unsloth/gpt-oss-20b", # 20B model using MXFP4 format  
    "unsloth/gpt-oss-120b",  
\] # More models at https://huggingface.co/unsloth  
  
model, tokenizer = FastLanguageModel.from\_pretrained(  
    model\_name = "unsloth/gpt-oss-20b",  
    dtype = None, # None for auto detection  
    max\_seq\_length = 4096, # Choose any for long context!  
    load\_in\_4bit = False,  # 4 bit quantization to reduce memory  
    full\_finetuning = False, # \[NEW!\] We have full finetuning now!  
    # token = "hf\_...", # use one if using gated models  
)  

### Reasoning Effort

The `gpt-oss` models from OpenAI include a feature that allows users to adjust the model's "reasoning effort." This gives you control over the trade-off between the model's performance and its response speed (latency) which by the amount of token the model will use to think.

The `gpt-oss` models offer three distinct levels of reasoning effort you can choose from:

*   **Low**: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.
*   **Medium**: A balance between performance and speed.
*   **High**: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency.

from transformers import TextStreamer  
  
messages = \[  
    {"role": "user", "content": "Solve x^5 + 3x^4 - 10 = 3."},  
\]  
inputs = tokenizer.apply\_chat\_template(  
    messages,  
    add\_generation\_prompt = True,  
    return\_tensors = "pt",  
    return\_dict = True,  
    reasoning\_effort = "low", # \*\*NEW!\*\* Set reasoning effort to low, medium or high  
).to(model.device)  
  
\_ = model.generate(\*\*inputs, max\_new\_tokens = 512, streamer = TextStreamer(tokenizer))  

<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-08-08

Reasoning: low

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Solve x^5 + 3x^4 - 10 = 3.<|end|><|start|>assistant<|channel|>analysis<|message|>Equation: x^5+3x^4-10=3 => x^5+3x^4-13=0. Solve maybe integer root? Try x=1:1+3-13=-9. x=2:32+48-13=39. x= -1: -1+3-13=-1. x=0: -13. So root between -1 and 2. Maybe rational 13 factors: ±1,13. Try x=13? huge. Try x= -13? huge. So only real root numeric. Use maybe substitute? We can factor x^4(x+3)-13=0. Not nice. So numeric approximation. Use maybe Newton. f(1.3)= 3.59+3\*2.85-13=3.59+8.55-13=-0.86. f(1.4): 1.4^5=1.4^2=1.96, \*1.4^3=1.96\*2.744=269? Let\'s compute precisely: 1.4^3=2.744; \*1.4^2=1.96 => 1.4^5=1.4^2\*1.4^3=1.96\*2.744= Cim =169.8? Wait compute: 1.96\*2.744=269?. Let\'s correct: 2.744\*2=5.488, minus 0.756 gives 4.732? This math messy. Let\'s use rough. Probably root ~1.37. Provide answer.<|end|><|start|>assistant<|channel|>final<|message|>The given equation can be written in the standard form  

\\\[
x^{5}+3x^{4}-13=0 .
\\\]

No integer or rational root satisfies this quintic equation (the
possible rational candidates \\( \\pm 1,\\pm 13\\) all give non‑zero
values), so the real root must be found numerically.

---

### Numerical evaluation  

Define  

\\\[
f(x)=x^{5}+3x^{4}-13 .
\\\]

The sign of \\(f(x)\\) changes between \\(x=-1\\) and \\(x=2\\):

\* \\(f(-1)=-1+3-13=-1

Changing the `reasoning_effort` to `medium` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer

from transformers import TextStreamer  
  
messages = \[  
    {"role": "user", "content": "Solve x^5 + 3x^4 - 10 = 3."},  
\]  
inputs = tokenizer.apply\_chat\_template(  
    messages,  
    add\_generation\_prompt = True,  
    return\_tensors = "pt",  
    return\_dict = True,  
    reasoning\_effort = "medium", # \*\*NEW!\*\* Set reasoning effort to low, medium or high  
).to(model.device)  
  
\_ = model.generate(\*\*inputs, max\_new\_tokens = 1024, streamer = TextStreamer(tokenizer))  

<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-08-08

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Solve x^5 + 3x^4 - 10 = 3.<|end|><|start|>assistant<|channel|>analysis<|message|>We need solve equation x^5 + 3x^4 - 10 = 3. Bring all terms: x^5 + 3x^4 - 13 = 0. Solve for x. It\'s a fifth degree polynomial, cannot be solved analytically in radicals nicely. But maybe find integer or rational roots? Try synthetic: possible integer rational roots from factors of 13: ±1, ±13. Evaluate:

x=1: 1+3-13=-9 => not zero.
x=-1: -1+3-13=-still -...-1+3=2-13=-11.
x=13: huge positive.
x=-13: negative huge.

Better factor? Let\'s check if any real root numeric approximate. Solve f(x)=x^5+3x^4-13=0. For large negative x, f negative? Let\'s evaluate f(-2)= (-32)+48-13=193? Wait compute: (-2)^5 = -32, 3\*(-2)^4=3\*16=48; sum=16-13=3. So f(-2)=3 positive. f(-3): (-243)+ 162-13=-194. So there\'s a root between -3 and -2. Let\'s also check positive side: f(1)=-9 negative; f(2):32+ 192-13= Mod? 32+192=224-13=211 positive. So a root between 1 and 2. Also any other roots? Fifth degree, so odd, so at least one real. But sign changes: f(-3) negative, f(-2) positive => one root ~ -2.something. f(-2)=3 positive, f(-1) negative? f(-1) = -1+3-13=-8 negative => yes another root between -2 and -1. But polynomial of degree 5 may have up to 5 roots, could have 3 or 5 real roots. Let\'s check f(0) = -13 negative. f(1) negative; f(2) positive => root between 1 and 2. f(3): 243+ 3\*81=243+243=486-13=473 positive, so no new root after 2 because sign positive remains. For negative large x: f(-4)=(-1024)+ 192-13=-, -14? Actually (-4)^5=-1024, 3\*256=768, sum=-, 768-1024=-, -? That\'s -, 768-1024=-, minus 13 => -, 768-1037=-, which is -, compute: -1024+768= -, -1, ok: -1024+768=-, 768-1024 = -, 768-1024 = -, 1024-768= 1? Actually compute precisely: -1024+768 = -, 1024-768 = 1??? Wait: -1024 + 768 = -, 1024-768 = 1? It would be -128? Let\'s comput...6979 bytes truncated.

